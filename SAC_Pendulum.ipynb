{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8be7a8b4-4156-4347-93f9-50bb1470e6c0",
      "metadata": {
        "id": "8be7a8b4-4156-4347-93f9-50bb1470e6c0"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class SAC(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, gamma=0.99, alpha=1e-3, tau=1e-2,\n",
        "                 batch_size=64, pi_lr=1e-3, q_lr=1e-3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(),\n",
        "                                      nn.Linear(128, 128), nn.ReLU(),\n",
        "                                      nn.Linear(128, 2 * action_dim), nn.Tanh())\n",
        "\n",
        "        self.q1_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(),\n",
        "                                      nn.Linear(128, 128), nn.ReLU(),\n",
        "                                      nn.Linear(128, 1))\n",
        "\n",
        "        self.q2_model = nn.Sequential(nn.Linear(state_dim + action_dim, 128), nn.ReLU(),\n",
        "                                      nn.Linear(128, 128), nn.ReLU(),\n",
        "                                      nn.Linear(128, 1))\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = []\n",
        "\n",
        "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
        "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
        "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
        "        self.q1_target_model = deepcopy(self.q1_model)\n",
        "        self.q2_target_model = deepcopy(self.q2_model)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action, _ = self.predict_actions(state)\n",
        "        return action.squeeze(1).detach().numpy()\n",
        "\n",
        "    def fit(self, state, action, reward, done, next_state):\n",
        "        self.memory.append([state, action, reward, done, next_state])\n",
        "\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            batch = random.sample(self.memory, self.batch_size)\n",
        "            states, actions, rewards, dones, next_states = map(torch.FloatTensor, zip(*batch))\n",
        "            rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
        "\n",
        "            next_actions, next_log_probs = self.predict_actions(next_states)\n",
        "            next_states_and_actions = torch.concatenate((next_states, next_actions), dim=1)\n",
        "            next_q1_values = self.q1_target_model(next_states_and_actions)\n",
        "            next_q2_values = self.q2_target_model(next_states_and_actions)\n",
        "            next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
        "            targets = rewards + self.gamma * (1 - dones) * (next_min_q_values - self.alpha * next_log_probs)\n",
        "\n",
        "            states_and_actions = torch.concatenate((states, actions), dim=1)\n",
        "            q1_loss = torch.mean((self.q1_model(states_and_actions) - targets.detach()) ** 2)\n",
        "            q2_loss = torch.mean((self.q2_model(states_and_actions) - targets.detach()) ** 2)\n",
        "            self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
        "            self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
        "\n",
        "            pred_actions, log_probs = self.predict_actions(states)\n",
        "            states_and_pred_actions = torch.concatenate((states, pred_actions), dim=1)\n",
        "            q1_values = self.q1_model(states_and_pred_actions)\n",
        "            q2_values = self.q2_model(states_and_pred_actions)\n",
        "            min_q_values = torch.min(q1_values, q2_values)\n",
        "            pi_loss = - torch.mean(min_q_values - self.alpha * log_probs)\n",
        "            self.update_model(pi_loss, self.pi_optimizer)\n",
        "\n",
        "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if model != None and target_model != None:\n",
        "            for param, terget_param in zip(model.parameters(), target_model.parameters()):\n",
        "                new_terget_param = (1 - self.tau) * terget_param + self.tau * param\n",
        "                terget_param.data.copy_(new_terget_param)\n",
        "\n",
        "    def predict_actions(self, states):\n",
        "        means, log_stds = self.pi_model(states).T\n",
        "        means, log_stds = means.unsqueeze(1), log_stds.unsqueeze(1)\n",
        "        dists = Normal(means, torch.exp(log_stds))\n",
        "        actions = dists.rsample()\n",
        "        log_probs = dists.log_prob(actions)\n",
        "        return actions, log_probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Pendulum-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "agent = SAC(state_dim, action_dim)\n",
        "\n",
        "total_rewards = []"
      ],
      "metadata": {
        "id": "rUX6RHRZRAiE"
      },
      "id": "rUX6RHRZRAiE",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d17058b-597f-4432-a2ea-ea13b2b2899f",
      "metadata": {
        "id": "7d17058b-597f-4432-a2ea-ea13b2b2899f"
      },
      "outputs": [],
      "source": [
        "episode_n = 15\n",
        "\n",
        "for episode in range(episode_n):\n",
        "    print(episode)\n",
        "\n",
        "    total_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(200):\n",
        "        action = agent.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(2 * action)\n",
        "\n",
        "        agent.fit(state, action, reward, done, next_state)\n",
        "\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "plt.plot(total_rewards)\n",
        "plt.title('total_rewards')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xlsHtjgqSd3F"
      },
      "id": "xlsHtjgqSd3F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}